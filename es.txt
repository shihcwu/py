###################################################
# 2018/12
# Elasticsearch/Kibana: Version: 6.5.2
# reference: https://www.elastic.co/
# reference: Udemy: Complete Guide to Elasticsearch
###################################################

# node: a server that stores data and is part of a cluster; default name is UUID (Universally Unique Identifier) 
# cluster: a collection of nodes (servers); default name is Elasticsearch
# document: each data item stored within a cluster is called document, being a basic unit of information that can be indexed; documents are stored within indices; documents are JSON objects
# index: an index is a collection of documents
# sharding divides indices into smaller pieces named shards; sharding enables you to distribute data across multiple nodes within a cluster; sharding also increses performance in cases where shards are distributed on multiple nodes because search queries can then be parallelized
# Elasticsearch natively support replication of your shards, meaning that shards are copied

# HTTP request methods
# GET: The GET method requests a representation of the specified resource. Requests using GET should only retrieve data.
# POST: The POST method is used to submit an entity to the specified resource, often causing a change in state or side effects on the server.
# PUT: The PUT method replaces all current representations of the target resource with the request payload.
# HTTP verb: GET, POST, PUT, DELETE
# <REST verb>/<Index>/<Type>/<API>     # <Type> will be removed in later/newer version of Elasticsearch

# create an index
PUT /<my_index_name>/

# add/replace a document to an index
POST /<my_index_name>/_doc
{
    "my_field_1": "my_value_1",
    "my_field_2": {
        "my_field_3": "my_value_2"
    }
}

# add/replace a document with _id to an index
PUT /<my_index_name>/_doc/<_id>
{
    "my_field_1": "my_value_1",
    "my_field_2": {
        "my_field_3": "my_value_2"
    }
}

# retrive a document by _id
GET /<my_index_name>/_doc/<_id>

# using update API to update a document
POST /<my_index_name>/_doc/<_id>/_update
{
    "doc": {"my_field_1": my_numeric_value, "my_field_new": ["my_value_1"]}
}

# scripted update; use "script" property instead of "doc" property
POST /<my_index_name>/_doc/<_id>/_update
{
    "script": "ctx._source.my_field_1 += 10"
}

# documents in Elasticsearch are immutable and cannot be changed

# upsert; if the document exist, then my_field_1 will be added by 5; if not exist, my_field_1 will be 100
POST /<my_index_name>/_doc/<_id>/_update
{
    "script": "ctx._source.my_field_1 += 5",
    "upsert": {
        "my_field_1": 100
    }

}

# delete a document
DELETE /<my_index_name>/_doc/<_id>

# use _delete_by_query API to delete multiple documents
POST /<my_index_name>/_delete_by_query
{
    "query": {
        "match": {
            "my_field_1": "my_value_1"
        }
    }
}

# bulk API can be used for add, update, and delete documents
# batch processing to add documents
POST /<my_index_name>/_doc/_bulk
{"index": {"_id": "100"}}
{"my_field_1": "my_value_1"}
{"index": {"_id": "101"}}
{"my_field_1": "my_value_1"}

# batch processing to update/delete documents
POST /<my_index_name>/_doc/_bulk
{"update": {"_id": "100"}}
{"doc": {"my_field_1": 1000}}
{"delete": {"_id": "101"}}

# import data with cURL
$ curl -H "Content-Type: application/json" -XPOST "http://localhost:9200/<my_index_name>/_doc/_bulk?pretty" --data-binary "@my_data_file.jsonl"

# use cat API for information about cluster, node, index, etc..
GET /_cat/health?v    # v for verbose
GET /_cat/nodes?v
GET /_cat/indices?v
GET /_cat/allocation?v    # how many shards are bing allocated
GET /_cat/shards?v

# In Elasticsearch, mappings are used to define how documents and their fields should be stored and indexed

GET /<my_index_name>/_doc/_mapping

# Each document has meta-data associated with it and they're called meta fields
# meta fields: _index, _id, _source, _field_names, _routing, _version, _meta
# field data types: core, complex, geo, specialized data types
# core: text, keyword (for filter/sort/aggregate), numeric, date, boolean (true/false), binary, range
# complex: object, array, nested    # Lucene has no concept of inner objects
# geo: geo-point, geo-shape
# specialize: ip, completion, attchment

# create an index with mappings
PUT /<my_index_name>/
{
    "mappings": {
        "my_default": {
            "dynamic": false,
            "properties": {
                "my_field": {
                    "type": "integer"
                },
                "my_field_2": {
                    "type": "boolean"
                }
            }
        }
    }
}

# mapping parameters: coerce (automatically cleaning up values), copy_to, dynamic, properties, norms (used for relevance scores), format (for date fields), null_value, fields

# add a mapping for new fields
PUT /<my_index_name>/_doc/_mapping
{
    "properties: {
        "my_field_new": {
            "type": "date",
            "format": "yyyy/MM/dd HH:mm:ss||yyyy/MM/dd"
        },
        "my_field_new_2": {
            "type": "text",
            "fields": {
                "keyword": {
                    "type": "keyword"
                }
            }
        }
    }"
}

# existing mappings for fields cannot be updated

# analysis process involves tokenizing and normalizing a block of text
# text is tokenized into terms and terms are converted into lower case letters (default behavior)
# results of analysis process are stored in inverted index
# inverted index is a mapping of a field's terms and which documents contain each term

# analyzer contains 1. character filter, 2. tokenizer, and 3. token filter
# character filter: manipulate text before tokenization; HTML strip character filter, mapping character filter, pattern replace
# tokenizer: split text into terms; word oriented tokenizers (standard, letter, lowercase, whitespace, UAX URL Email), partial work tokenizers (N-Gram, Edge N-Gram), structured text tokenizers (keyword, pattern, path)
# token filter: manipulate terms before adding them to an inverted index; standard, lowercase, uppercase, NGram, Edge NGram, stop, word delimiter, stemmer (flew -> fly), keyword marker, snowball, synonym, ASCII folding (résumé -> resume))
# built-in analyzer: standard, simple, stop, language, keyword, pattern, whitespace

# analyze API
POST _analyze
{
    "tokenizer": "standard",
    "filter": ["lowercase", "unique", "asciifolding"],
    "text": "My@email.com mY.email my_email my-email résumé"
}

# define a custome analyzer which uses standard analyzer (configure built-in analyzers and token filters)
PUT /my_analyzer
{
    "settings": {
        "analysis": {
            "analyzer": {
                "my_english_stop": {
                    "type": "standard",
                    "stopwords": "_english_"
                }
            },
            "filter": {
                "my_stemmer": {
                    "type": "stemmer",
                    "name": "english"
                }
            }
        }
    }
}

POST /my_analyzer/_analyze
{
  "analyzer": "my_english_stop",
  "text": "I'm in the mood of drinking semi-dry red wine!"
}

POST /my_analyzer/_analyze
{
  "tokenizer": "standard",
  "filter": ["my_stemmer"],
  "text": "I'm in the mood of drinking semi-dry red wine!"
}

PUT /my_analyzer_2
{
    "settings": {
        "analysis": {
            "analyzer": {
                "my_english_stop": {
                    "type": "standard",
                    "stopwords": "_english_"
                },
                "my_custom_analyzer": {
                    "type": "custom",
                    "tokenizer": "standard",
                    "char_filter": [
                        "html_strip"
                    ],
                    "filter": [
                        "standard",
                        "lowercase",
                        "trim",
                        "my_stemmer"
                    ]
                }
            },
            "filter": {
                "my_stemmer": {
                    "type": "stemmer",
                    "name": "english"
                }
            }
        }
    }
}

POST /my_analyzer_2/_analyze
{
    "analyzer": "my_custom_analyzer",
    "text": "I'm in the mood for drinking <strong> semi-dry </strong> red wine! go went fly flew"
}

# use analyzer in mapping
PUT /my_analyzer_2/_doc/_mapping
{
    "properties": {
        "my_field_1": {
            "type": "text",
            "analyzer": "my_custom_analyzer"
        },
        "my_field_2": {
            "type": "text",
            "analyzer": "standard"
        },
        "my_fields_3": {
            "type": "text"
        }
    }
}

POST /my_analyzer_2/_doc/1
{
    "my_field_1": "drinking",
    "my_field_2": "drinking"
}

# query below will not hit any documents, since my_field_1 is stemmed to drink
GET /my_analyzer_2/_search
{
  "query": {
    "term": {
      "my_field_1": "drinking"
    }
  }
}

# add analyzers to existing indices (close index and then add analyzers, and then open index)

POST /my_analyzer_2/_close

PUT /my_analyzer_2/_settings
{
    "analysis": {
        "analyzer": {
            "my_french_stop": {
                "type": "standard",
                "stopwords": "_french_"
            }
        }
    }
}

POST /my_analyzer_2/_open

# the relevance scoring algorithm, OkAPI BM25, is currently used by Elasticsearch (it considers term frequency, inverse document frequency, field-length norm)

GET /<my_index_name>/_search
{
    "explain": true,
    "query": {
        "match": {
            "my_field": "my_value"
        }
    }
}

GET /<my_index_name>/_doc/_id/_explain
{
  "query": {
    "match": {
      "my_field": "my_value"
    }
  }
}
